# -*- coding: utf-8 -*-
"""akshitbert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FlQMnEgi0XOkf4xiero9XdaJ4jesgsIT
"""

!pip install sentencepiece transformers datasets

import pandas as pd

with open("test1.txt", "r") as f:
    lines = f.read()
    
lines_array = lines.split("\n")
lines_array.pop()
test_split = [line.split("\t") for line in lines_array]

with open("train1.txt", "r") as f:
    lines = f.read()
    
lines_array = lines.split("\n")
lines_array.pop()
train_split = [line.split("\t") for line in lines_array]
    
with open("dev1.txt", "r") as f:
    lines = f.read()

lines_array = lines.split("\n")
lines_array.pop()
dev_split = [line.split("\t") for line in lines_array]

df = {}
df["test"] = pd.DataFrame(test_split, columns=["input", "output"])
df["train"] = pd.DataFrame(train_split, columns=["input", "output"])
df["dev"] = pd.DataFrame(dev_split, columns=["input", "output"])

from datasets import load_dataset, Dataset, DatasetDict

ds = DatasetDict()
ds["test"] = Dataset.from_pandas(df["test"])
ds["train"] = Dataset.from_pandas(df["train"])
ds["dev"] = Dataset.from_pandas(df["dev"])

ds

def tokenise_ds(example):
    return tokenizer.prepare_seq2seq_batch(src_texts=example["input"], tgt_texts=example["output"])

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("ai4bharat/IndicBART")
tokenizer.model_max_length = 64

def tokenise_ds(example):
    return tokenizer.prepare_seq2seq_batch(src_texts=example["input"], tgt_texts=example["output"])

    
tokenized_datasets = ds.map(tokenise_ds, batched=True)

from transformers import DataCollatorWithPadding, AutoModelForSeq2SeqLM, BertForSequenceClassification, TrainingArguments, Trainer, DataCollatorForSeq2Seq

model = AutoModelForSeq2SeqLM.from_pretrained("ai4bharat/IndicBART")
# model = BertForSequenceClassification.from_pretrained("ai4bharat/indic-bert")
data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, model=model)

from transformers import Trainer

from transformers import TrainingArguments

# training_args = TrainingArguments("test-trainer")
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=12,
    per_device_train_batch_size=16,
    learning_rate=0.001,
)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
#    train_dataloader=train_dataloader,
    eval_dataset=tokenized_datasets["dev"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()

trainer.evaluate()

ds["test"][0]["input"]

input_ids = tokenizer("absolutely epic scenes", return_tensors="pt").input_ids  # Batch size 1

outputs = model.generate(input_ids.cuda())
print(tokenizer.decode(outputs[0]))

with open("test2.txt", "r") as f:
    lines = f.read()
    
lines_array = lines.split("\n")
lines_array.pop()
test_split = [line.split("\t") for line in lines_array]
df["test"] = pd.DataFrame(test_split, columns=["input", "output"])

ds["test"] = Dataset.from_pandas(df["test"])

test_tokens = [tokenizer(sent, return_tensors="pt").input_ids for sent in ds["test"]["input"]]
output_sent = []
for idx, sent in enumerate(ds["test"]["input"]):
  sent_token = tokenizer(sent, return_tensors="pt").input_ids
  output = model.generate(sent_token.cuda())
  output_sent.append(tokenizer.decode(output[0], skip_special_tokens=True))
  if idx % 10 == 0:
    print(idx)

weights = [(1, 0, 0, 0), (0.5, 0.5), (0.33, 0.33, 0.33, 0), (0.25, 0.25, 0.25, 0.25),(0.2,0.2,0.2,0.2,0.2),(0.16,0.16,0.16,0.16,0.16,0.16),(0.14,0.14,0.14,0.14,0.14,0.14,0.14),(0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125)]

from nltk.translate.bleu_score import sentence_bleu
def bleu_score(references, candidates):
  score = [0]*len(weights)
  for i in range(len(references)):
      for j,w in enumerate(weights):
          temp_score = sentence_bleu([references[i]], candidates[i], weights=w)
          # add temp score truncated to 2 decimal places to score[j]
          score[j] += temp_score
  for i in range(len(score)):
      score[i] = score[i]/len(references)
      score[i] = round(score[i],6)
  return score

def evaluate(validation_data,translated_sentence):
    predicted_sentences = []
    for i in translated_sentence:
        predicted_sentences.append(i.split())

    target_sentences = []
    for i in validation_data:
        target_sentences.append(i.split())
    score = bleu_score(target_sentences,predicted_sentences)
    return(score)

score = evaluate(ds["test"]["output"], output_sent)

!pip install wandb

import wandb
wandb.login()

wandb.init(entity = "lakshmipathi-balaji",   # wandb username. (NOT REQUIRED ARG. ANYMORE, it fetches from initial login)
           project = "inlp-project", # wandb project name. New project will be created if given project is missing.
)
data_to_log={         
  "bleu_score_test": net_score_test,
  "bleu_score_unigram" : score[0],
  "bleu_score_bigram" : score[1],
  "bleu_score_trigram" : score[2],
  "bleu_score_4gram" : score[3],
  "bleu_score_5gram" : score[4],
  "bleu_score_6gram" : score[5],
  "bleu_score_7gram" : score[6],
  "bleu_score_8gram" : score[7]
}
wandb.log(data_to_log)