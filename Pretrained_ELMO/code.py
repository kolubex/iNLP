# -*- coding: utf-8 -*-
"""inlp_ass4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kEIBXyOYivpBBgB6UgUwuAz9CwbXSu12
"""

import nltk
nltk.download('punkt')
from nltk.stem import PorterStemmer
nltk.download('stopwords')
from nltk.corpus import stopwords
import csv
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import re
from sklearn.metrics import classification_report
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# variables
PAD_TOKEN = '<PAD>'
PAD_TOKEN_IDX = 0
UNK_TOKEN = '<UNK>'
UNK_TOKEN_IDX = 1
SOS_TOKEN = '<SOS>'
SOS_TOKEN_IDX = 2
EOS_TOKEN = '<EOS>'
EOS_TOKEN_IDX = 3

# Hyperparameters
EMBEDDING_DIM = 100
HIDDEN_DIM = 128
NUM_LAYERS = 3
LEARNING_RATE = 0.01
EPOCHS = 20
BATCH_SIZE = 32
SEQUENCE_LENGTH = 20

class Elmo(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(Elmo, self).__init__()
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = NUM_LAYERS
        
        # Pre-trained word embeddings layer
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim,padding_idx=PAD_TOKEN_IDX)
        
        # Load pre-trained word embeddings
        # self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)

        # Stacked bidirectional LSTM layers
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)
        # a linear layer for forward word prediction
        self.linear_forward = nn.Linear(hidden_dim, vocab_size)

        # a linear layer for backward word prediction
        self.linear_backward = nn.Linear(hidden_dim, vocab_size)

    

    def forward(self, input_ids):
        # Input_ids: list of lists where each inner list represents a sentence
        layer_wise_hidden_states = []
        # Word embeddings
        embedded = self.word_embeddings(input_ids)
        embedded.to(device)
        # initialize the hidden state and cell state of the LSTM
        _ = (torch.zeros(2, self.hidden_dim).to(device), torch.zeros(2,self.hidden_dim).to(device))
        # Bidirectional LSTM layers
        for i in range(self.num_layers):
            lstm_output, _ = self.lstm(embedded,_)
            layer_wise_hidden_states.append(lstm_output.detach())
        lstm_output = lstm_output.view(len(input_ids), -1, self.hidden_dim*2)
        
        # last hidden_state of the LSTM
        last_hidden_state = lstm_output[:, -1, :]
        # print(lstm_output.shape)
        # pass the forward part of last hidden state through the linear layer for forward word prediction
        forward_prediction = self.linear_forward(last_hidden_state[:, :self.hidden_dim])

        # pass the backward part of last hidden state through the linear layer for backward word prediction
        backward_prediction = self.linear_backward(last_hidden_state[:, self.hidden_dim:])

        # apply log_softmax on the forward and backward word predictions
        forward_prediction = F.log_softmax(forward_prediction, dim=1)
        backward_prediction = F.log_softmax(backward_prediction, dim=1)
        # print(len(layer_wise_hidden_states), layer_wise_hidden_states[0].shape)
        embedded = embedded.detach()
        return embedded,layer_wise_hidden_states, forward_prediction, backward_prediction

import torch
from torch.utils.data import Dataset, DataLoader

# Define a custom Dataset class
class MyDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        sentence = self.data[index]
        input_ids = torch.tensor(sentence, dtype=torch.long).to(device)
        return input_ids

class data_preperation():
    def __init__(self, train_file, val_file, test_file, sample_file):
        self.train_file = train_file
        self.val_file = val_file
        self.test_file = test_file
        self.sample_file = sample_file
    
    def get_corpus_sst(self, file_name):
        # read the csv file
        df = pd.read_csv(file_name)
        # get the sentence column
        sentence_coloumn = df['sentence']
        sentences = []
        for i in range(len(sentence_coloumn)):
            # get the text
            text = sentence_coloumn[i]
            # remove punctuations
            text = re.sub(r'(!|"|\#|\$|%|&|\'|\(|\)|\*|\+|,|-|—|’|\.|\/|:|;|<|=|>|\?|@|\[|\\|\]|\^|_|‘|\{|\||\}|~{1,})', r' ', text)
            # tokenize the text
            tokens = nltk.word_tokenize(text)
            # stem the tokens
            ps = PorterStemmer()
            tokens = [ps.stem(token) for token in tokens]
            # remove the stop words
            stop_words = set(stopwords.words('english'))

            tokens = [token for token in tokens if token.lower() not in stop_words]
            if(len(tokens)):
                sentences.append(tokens)
        return sentences

    def get_data_sst(self, file_name):
        # read the csv file
        df = pd.read_csv(file_name)
        # get the sentence column
        sentence_coloumn = df['sentence']
        label_coloumn = df['label']
        sentences_tuple = []
        for i in range(len(sentence_coloumn)):
            # get the text
            text = sentence_coloumn[i]
            # get the lable
            label = float(label_coloumn[i])
            # truncate the label  to 2 decimal places
            label = round(label, 1)
            # remove punctuations
            text = re.sub(r'(!|"|\#|\$|%|&|\'|\(|\)|\*|\+|,|-|—|’|\.|\/|:|;|<|=|>|\?|@|\[|\\|\]|\^|_|‘|\{|\||\}|~{1,})', r' ', text)
            # tokenize the text
            tokens = nltk.word_tokenize(text)
            # stem the tokens
            ps = PorterStemmer()
            tokens = [ps.stem(token) for token in tokens]
            # remove the stop words
            stop_words = set(stopwords.words('english'))

            tokens = [token for token in tokens if token.lower() not in stop_words]
            if(len(tokens)):
                sentences_tuple.append((tokens, label))
        return sentences_tuple
    
    def get_corpus_mli(self, file_name):
        # read the csv file
        df = pd.read_csv(file_name)
        # get the sentence column
        sentence_coloumn = df['premise', 'hypothesis']
        sentences = []
        for i in range(len(sentence_coloumn)):
            # get the text
            text = sentence_coloumn[i]
            # tokenize the text
            tokens = nltk.word_tokenize(text)
            # stem the tokens
            ps = PorterStemmer()
            tokens = [ps.stem(token) for token in tokens]
            # remove the stop words
            stop_words = set(stopwords.words('english'))
            tokens = [token for token in tokens if token.lower() not in stop_words]
            sentences.append(tokens)
        return sentences

# Sample data
def main():
    final_lstm_hidden_states = []
    data_maker = data_preperation(train_file='./train.csv', val_file='./sample.csv', test_file='./test.csv', sample_file='./sample.csv')
    sentences = data_maker.get_corpus_sst(data_maker.sample_file)

    # Preprocessing the data
    vocab = set([word for sentence in sentences for word in sentence])
    word_to_ix = {PAD_TOKEN:PAD_TOKEN_IDX,UNK_TOKEN:UNK_TOKEN_IDX,SOS_TOKEN:SOS_TOKEN_IDX,EOS_TOKEN:EOS_TOKEN_IDX}
    for i, word in enumerate(vocab):
        word_to_ix[word] = i+4
    ix_to_word = {}
    for word, ix in word_to_ix.items():
        ix_to_word[ix] = word
    data = []
    for sentence in sentences:
        sent_ix = [word_to_ix[word] for word in sentence]
        if len(sent_ix) < SEQUENCE_LENGTH:
            sent_ix = sent_ix + [PAD_TOKEN_IDX for i in range(SEQUENCE_LENGTH-len(sent_ix))]
        else:
            sent_ix = sent_ix[:SEQUENCE_LENGTH]
        sent_ix = [SOS_TOKEN_IDX] + sent_ix + [EOS_TOKEN_IDX]
        sent_ix = torch.tensor(sent_ix, dtype=torch.long).to(device)
        data.append(sent_ix)
    
    # divide the data into batches
    dataset = MyDataset(data)
    batched_data = DataLoader(dataset, batch_size=BATCH_SIZE)

    VOCAB_SIZE = len(vocab)+4

    # Instantiating the model
    elmo_model = Elmo(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM)
    elmo_model.to(device)

    # Defining the loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(elmo_model.parameters())

    # Training loop
    for epoch in range(EPOCHS):
        total_loss = 0
        final_lstm_hidden_states = []
        normal_embeddings = []
        for batch in batched_data:
            # pass the sentece to the elmo model
            batch_loss = 0
            for sentence in batch:
                sentence = torch.tensor(sentence).to(device)
                embedding,all_hidden_states, forward_prediction, backward_prediction = elmo_model(sentence)
                normal_embeddings.append(embedding)
                final_lstm_hidden_states.append(all_hidden_states)
                # calculate the loss
                batch_loss += criterion(forward_prediction, sentence)
                # invert the sentence
                sentence = sentence.flip(0)
                batch_loss += criterion(backward_prediction, sentence)

            total_loss += batch_loss.item()
            # backpropagation
            batch_loss.backward()
            optimizer.step()
            optimizer.zero_grad()
        print("Epoch: {}, Loss: {}".format(epoch, total_loss))
    
    sentences_test = data_maker.get_corpus_sst(data_maker.sample_file)
    vocab_test = set([word for sentence in sentences_test for word in sentence])
    word_to_ix_test = {PAD_TOKEN:PAD_TOKEN_IDX,UNK_TOKEN:UNK_TOKEN_IDX,SOS_TOKEN:SOS_TOKEN_IDX,EOS_TOKEN:EOS_TOKEN_IDX}
    for i, word in enumerate(vocab_test):
        if word in word_to_ix:
            word_to_ix_test[word] = word_to_ix[word]
        else:
            word_to_ix_test[word] = UNK_TOKEN_IDX
    ix_to_word_test = {}
    for word, ix in word_to_ix_test.items():
        ix_to_word_test[ix] = word
    data_test = []
    for sentence in sentences_test:
        sent_ix = [word_to_ix_test[word] for word in sentence]
        if len(sent_ix) < SEQUENCE_LENGTH:
            sent_ix = sent_ix + [PAD_TOKEN_IDX for i in range(SEQUENCE_LENGTH-len(sent_ix))]
        else:
            sent_ix = sent_ix[:SEQUENCE_LENGTH]
        sent_ix = [SOS_TOKEN_IDX] + sent_ix + [EOS_TOKEN_IDX]
        sent_ix = torch.tensor(sent_ix, dtype=torch.long).to(device)
        data_test.append(sent_ix)
    
    # divide the data into batches
    dataset_test = MyDataset(data_test)
    batched_data_test = DataLoader(dataset_test, batch_size=BATCH_SIZE)
    
    # Instantiating the model
    elmo_model_test = Elmo(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM)
    elmo_model_test.to(device)
    # Defining the loss and optimizer
    criterion_test = nn.CrossEntropyLoss()
    optimizer_test = torch.optim.Adam(elmo_model_test.parameters())

    # Training loop
    for epoch in range(EPOCHS):
        total_loss = 0
        final_lstm_hidden_states_test = []
        normal_embeddings_test = []
        for batch in batched_data_test:
            # pass the sentece to the elmo model
            batch_loss = 0
            for sentence in batch:
                sentence = torch.tensor(sentence)
                embedding,all_hidden_states, forward_prediction, backward_prediction = elmo_model_test(sentence)
                normal_embeddings_test.append(embedding)
                final_lstm_hidden_states_test.append(all_hidden_states)
                # calculate the loss
                batch_loss += criterion_test(forward_prediction, sentence)
                # invert the sentence
                sentence = sentence.flip(0)
                batch_loss += criterion_test(backward_prediction, sentence)

            total_loss += batch_loss.item()
            # backpropagation
            batch_loss.backward()
            optimizer_test.step()
            optimizer_test.zero_grad()
        print("Epoch: {}, Loss: {}".format(epoch, total_loss))
    return normal_embeddings,final_lstm_hidden_states,elmo_model,normal_embeddings_test,final_lstm_hidden_states_test,elmo_model_test

from torch.nn.functional import normalize as l2_norm
class SST_MODEL(nn.Module):
    # give me a lstm model that is used for classification of a sentence
    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers):
        super(SST_MODEL, self).__init__()
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,bidirectional=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        # a linear function to linearly interpolate the hidden states that are passed for every element
        # where the shape is [sequence_length, hidden_dim*2]
        self.interpolation_parameters = nn.parameter.Parameter(torch.zeros(NUM_LAYERS))
        self.normalize = nn.Softmax()
        
    
    def forward(self, normal_embeddings, hidden_states):
        # text = [sent len, batch size]
        # embedded = [sent len, batch size, emb dim]
        # elmo_embedded = linear_interpolation(normal_embeddings, hidden_states)
        # make a tensor of shape hidden_states[0]
        hidden_state_final = torch.zeros_like(hidden_states[0])
        for i,hidden_state in enumerate(hidden_states):
            hidden_state_final += hidden_state*self.interpolation_parameters[i]
        
        # print("interpolation_parameters: ",self.interpolation_parameters)
        elmo_embedded = normal_embeddings
        elmo_embedded = torch.cat((elmo_embedded, hidden_state_final), dim=1)
        
        lstm_output, _ = self.lstm(elmo_embedded)
        # print("lstm output: ",lstm_output.shape, "hidden_shape: ",_[0].shape)
        output_all = _[0][-1]
        # output = [sent len, batch size, hid dim * num directions]
        output_all = self.fc(output_all)
        output_all = F.log_softmax(output_all)
        # hidden = [batch size, hid dim * num directions]
        # print("output: ",output_all.shape)
        return output_all

normal_embeddings,layer_wise_hidden_states,elmo_model,normal_embeddings_test,layer_wise_hidden_states_test,elmo_model_test = main()

def save_model_sst(model, path):
    torch.save(model.state_dict(), path)
    print("Model saved at {}".format(path))
def sst(normal_embeddings,layer_wise_hidden_states,elmo_model,normal_embeddings_test,layer_wise_hidden_states_test,elmo_model_test ):
    # Hyperparameters
    EMBEDDING_DIM = 100 + 2*128
    HIDDEN_DIM = 128
    NUM_LAYERS = 2
    LEARNING_RATE = 0.01
    EPOCHS = 40
    BATCH_SIZE = 64
    SEQUENCE_LENGTH = 20
    # print("total_embeddings: ",(layer_wise_hidden_states[0]))
    data_maker = data_preperation(train_file='./train.csv', val_file='./sample.csv', test_file='./test.csv', sample_file='./sample.csv')
    sentences_lable = data_maker.get_data_sst(data_maker.sample_file)
    # Preprocessing the data
    vocab = set([word for sentence in sentences_lable for word in sentence[0]])
    word_to_ix = {PAD_TOKEN:PAD_TOKEN_IDX,UNK_TOKEN:UNK_TOKEN_IDX,SOS_TOKEN:SOS_TOKEN_IDX,EOS_TOKEN:EOS_TOKEN_IDX}
    for i, word in enumerate(vocab):
        word_to_ix[word] = i+4
    ix_to_word = {}
    for word, ix in word_to_ix.items():
        ix_to_word[ix] = word
    
    set_labels = set([sentence[1] for sentence in sentences_lable])
    # make label_to_idx
    label_to_ix = {}
    for i, label in enumerate(set_labels):
        label_to_ix[label] = i
    ix_to_label = {}
    for label, ix in label_to_ix.items():
        ix_to_label[ix] = label
    data = []
    label = []
    for sentence in sentences_lable:
        label.append(float(sentence[1]))
        sentence = sentence[0]
        sent_ix = [word_to_ix[word] for word in sentence]
        if len(sent_ix) < SEQUENCE_LENGTH:
            sent_ix = sent_ix + [PAD_TOKEN_IDX for i in range(SEQUENCE_LENGTH-len(sent_ix))]
        else:
            sent_ix = sent_ix[:SEQUENCE_LENGTH]
        sent_ix = [SOS_TOKEN_IDX] + sent_ix + [EOS_TOKEN_IDX]
        sent_ix = torch.tensor(sent_ix, dtype=torch.long).to(device)
        data.append((sent_ix))
    # divide the data into batches
    dataset = MyDataset(data)
    batched_data = DataLoader(dataset, batch_size=BATCH_SIZE)

    # Instantiating the model sst
    sst_model = SST_MODEL(EMBEDDING_DIM, HIDDEN_DIM, len(set_labels), NUM_LAYERS)
    sst_model.to(device)

    # Defining the loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(sst_model.parameters())
    # Training loop
    for epoch in range(EPOCHS):
        sentence_num = 0
        total_loss = 0
        for batch in batched_data:
            # pass the sentece to the elmo model
            batch_loss = 0
            # optimizer.zero_grad()
            # print("batch: ",batch.shape)
            for sentence in batch:
                prob = sst_model(normal_embeddings[sentence_num],layer_wise_hidden_states[sentence_num])
                # label[sentence_num] = torch.tensor(label[sentence_num])
                # print("prob: ",prob.shape,"label: ",label_to_ix[label[sentence_num]])
                batch_loss += criterion(prob,torch.tensor(label_to_ix[label[sentence_num]]).to(device))
                sentence_num+=1
            batch_loss.backward()
            # print("batch_loss: ",batch_loss)
            optimizer.step()
            optimizer.zero_grad()
            total_loss += batch_loss.item()
        print("Epoch: {}, Loss: {}".format(epoch, total_loss))
    
    save_model_sst(sst_model, './sst_model.pt')
    # Testing loop
    sentences_lable_test = data_maker.get_data_sst(data_maker.sample_file)
    # Preprocessing the data
    vocab_test = set([word for sentence in sentences_lable_test for word in sentence[0]])
    word_to_ix_test = {PAD_TOKEN:PAD_TOKEN_IDX,UNK_TOKEN:UNK_TOKEN_IDX,SOS_TOKEN:SOS_TOKEN_IDX,EOS_TOKEN:EOS_TOKEN_IDX}
    for i, word in enumerate(vocab_test):
        if word in word_to_ix:
            word_to_ix_test[word] = word_to_ix[word]
        else:
            word_to_ix_test[word] = word_to_ix[UNK_TOKEN]
    ix_to_word_test = {}
    for word, ix in word_to_ix_test.items():
        ix_to_word_test[ix] = word
    
    set_labels_test = set([sentence[1] for sentence in sentences_lable_test])
    # print(set_labels_test)
    # print(set_labels)
    # make label_to_idx
    label_to_ix_test = {}
    for i, label in enumerate(set_labels_test):
        if label in label_to_ix:
            label_to_ix_test[label] = label_to_ix[label]
        else:
            label_to_ix_test[label] = len(label_to_ix_test)
    # print(label_to_ix_test)
    # print(label_to_ix)
    ix_to_label_test = {}
    for label, ix in label_to_ix_test.items():
        ix_to_label_test[ix] = label
    data_test = []
    label_test = []
    for sentence in sentences_lable_test:
        label_test.append(float(sentence[1]))
        sentence = sentence[0]
        sent_ix = [word_to_ix_test[word] for word in sentence]
        if len(sent_ix) < SEQUENCE_LENGTH:
            sent_ix = sent_ix + [PAD_TOKEN_IDX for i in range(SEQUENCE_LENGTH-len(sent_ix))]
        else:
            sent_ix = sent_ix[:SEQUENCE_LENGTH]
        sent_ix = [SOS_TOKEN_IDX] + sent_ix + [EOS_TOKEN_IDX]
        sent_ix = torch.tensor(sent_ix, dtype=torch.long).to(device)
        data_test.append((sent_ix))
    # print("labels: ",label_test)
    label_test_indexed = [label_to_ix[label1] for label1 in label_test]
    predicted_labels = []
    with torch.no_grad():
        sst_model.eval()
        sentence_num = 0
        total_loss = 0
        for sentence in data_test:
            prob = sst_model(normal_embeddings[sentence_num],layer_wise_hidden_states[sentence_num])
            # take out the label with the highest probability
            _,highest_prob_label = torch.max(prob,0)
            # print(highest_prob_label,prob)
            predicted_labels.append(highest_prob_label.item())
            # print("label: ",label_to_ix_test[label_test[sentence_num]],"highest_prob_label: ",highest_prob_label)
            sentence_num+=1
    print(classification_report(label_test_indexed, predicted_labels))
    
    return sst_model,ix_to_label_test,label_to_ix_test,ix_to_label,label_to_ix,label_test_indexed,predicted_labels

a = sst(normal_embeddings,layer_wise_hidden_states,elmo_model,normal_embeddings_test,layer_wise_hidden_states_test,elmo_model_test )

