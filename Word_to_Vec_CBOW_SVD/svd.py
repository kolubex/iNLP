# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WkKTb_p2kN1OXz5cfkKWEs2sLZlcsFCO
"""

import os
import json
import re
from nltk.tokenize import word_tokenize
from collections import defaultdict
import numpy as np
import torch.nn as nn
import torch
from sklearn.decomposition import TruncatedSVD

class tokenization:
    def __init__(self):
        pass

    def replaceHashtags(self, txt):
        return re.sub('\#[a-zA-Z]\w+', '', txt)

    def replace_email(self, corpus):
        return re.sub(r'\S*@\S*\s?', r'', corpus)

    def replaceURL(self, txt):
        return re.sub(r'(https?:\/\/|www\.)?\S+[a-zA-Z0-9]{2,}\.[a-zA-Z0-9]{2,}\S+', r'', txt)

    def replaceMentions(self, txt):
        return re.sub(r'@\w+', r'', txt)

    def replaceDateTime(self, txt):
        txt = re.sub(
            r'\d{2,4}\-\d\d-\d{2,4}|\d{2,4}\/\d\d\/\d{2,4}|\d{2,4}:\d\d:?\d{2,4}', '', txt)
        return re.sub(r'\d+:\d\d:?\d{0,2}?( am|am| pm|pm)', r'', txt)

    # for better results as we go by string comparisions
    def upperToLower(self, txt): return txt.lower()


    def replacePunctuation(self, txt): return re.sub(
        r'(!|"|\#|\$|%|&|\'|\(|\)|\*|\+|,|-|—|’|\.|\/|:|;|<|=|>|\?|@|\[|\\|\]|\^|_|‘|\{|\||\}|~{1,})', r'', txt)

    def replaceMobileNumber(self, txt):
        return re.sub(r'[\+0-9\-\(\)\.]{3,}[\-\.]?[0-9\-\.]{3,}', r'', txt)

    def replaceNumericals(self, txt):
        # returning the numericals as <NUM> as we are not considering the numericals in the corpus
        return re.sub(r'\d+', r'', txt)

def tokenize(corpus):
    token = tokenization()
    corpus = token.replaceHashtags(corpus)
    corpus = token.replace_email(corpus)
    corpus = token.replaceURL(corpus)
    corpus = token.replaceMentions(corpus)
    corpus = token.upperToLower(corpus)
    corpus = token.replaceDateTime(corpus)
    corpus = token.replacePunctuation(corpus)
    corpus = token.replaceMobileNumber(corpus)
    corpus = token.replaceNumericals(corpus)
    return corpus.split()

# main function
if __name__ == "__main__":
    sentences = []
    sentences_tokenized = []
    # Define a defaultdict to store the co-occurrence counts
    co_occurrence = defaultdict(int)
    window_size = 1
    num_dimensions = 100
    with open("test.json") as f:
        index = 0
        for line in f:
            review = json.loads(line)
            line = line.split(""""reviewText":""")[1]
            line = line.split(""", "overall":""")[0]
            # get sentences from the line
            line = re.split(r' *[\.\?!][\'"\)\]]* *', line)
            for sentence in line:
                if(len(sentence) > 0):
                    sentences.append(sentence)
                    sentences_tokenized.append(tokenize(sentence))
                if(len(sentences_tokenized)>40000):
                    index = 1
                    break
            if(index == 1):
                break

    # for sentence in sentences_tokenized:
    #     for i, word in enumerate(sentence):
    #         for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):
    #             if i != j:
    #                 co_occurrence[(word, sentence[j])] += 1

    # Create a vocabulary
    vocab = list(set([word for sentence in sentences_tokenized for word in sentence]))


    # # Initialize the co-occurrence matrix
    matrix = np.zeros((len(vocab), len(vocab)))
    word_to_id = {word: i for i, word in enumerate(vocab)}
    for sentence in sentences_tokenized:
        for i, word in enumerate(sentence):
            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):
                if i != j:
                    matrix[word_to_id[word], word_to_id[sentence[j]]] += 1

    # # Perform SVD on the co-occurrence matrix
    svd = TruncatedSVD(n_components=50)
    word_vectors_svd = svd.fit_transform(matrix)
    # Normalize the word vectors
    norms = np.linalg.norm(word_vectors_svd, axis=1, keepdims=True)
    norms[norms == 0] = 1e-8 # replace any zero values with a small value
    word_vectors_normalized = word_vectors_svd / norms
    # Create a dictionary of word vectors
    word_vectors_dict = {}
    for i, word in enumerate(vocab):
        word_vectors_dict[word] = word_vectors_normalized[i]

# print all the embeddings into a file for further use using dict
with open('embeddings.txt', 'w') as file:
    for key, value in word_vectors_dict.items():
        file.write(str(key) + ' ' + str(value) + ' ')

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
tsne = TSNE(n_components=2, random_state=0)
# np.set_printoptions(suppress=True)
Y = tsne.fit_transform(word_vectors_normalized)
labels = vocab

# Define the word for which to find similar words
word = "award"

# Find the 10 most similar words to the given word
most_similar = [vocab[i] for i in np.argsort(np.linalg.norm(word_vectors_normalized - word_vectors_normalized[vocab.index(word)], axis=1))[1:11]]

# Plot the embeddings, highlighting the given word and its 10 most similar words
plt.figure(figsize=(14, 14))
for label, x, y in zip(labels, Y[:, 0], Y[:, 1]):
    if label == word:
        plt.scatter(x, y, c='r')
        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')
    elif label in most_similar:
        plt.scatter(x, y, c='b')
        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')
        
plt.show()

# Define the word for which to find similar words
word = "titanic"

# Find the 10 most similar words to the given word
most_similar = [vocab[i] for i in np.argsort(np.linalg.norm(word_vectors_normalized - word_vectors_normalized[vocab.index(word)], axis=1))[1:21]]
for a in most_similar:
    print(a)